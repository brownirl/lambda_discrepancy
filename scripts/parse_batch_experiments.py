"""
Parse experiment directory from batch_ppo_run.py.
We assume that things are run based off of the runs generated by the files in
scripts/hyperparams.
This file amalgamates all results into a single (huge) tensor, and saves the result
in a file.

After running this, you should run either best_hyperparams.py (for best hyperparams across all envs)
or best_hyperparams_per_env.py (for best hyperparams for each env), to generate
a file that has the best hyperparams, and the best score.
"""
import argparse
from collections import OrderedDict
import importlib
from pathlib import Path
import pickle
import sys

import jax.numpy as jnp
import orbax.checkpoint
import numpy as np
from tqdm import tqdm

from definitions import ROOT_DIR


def get_total_size(obj, seen=None):
    """Recursively finds size of objects in bytes."""
    size = sys.getsizeof(obj)
    if seen is None:
        seen = set()

    # Add object's id to seen to avoid double counting of objects
    obj_id = id(obj)
    if obj_id in seen:
        return 0
    seen.add(obj_id)

    if isinstance(obj, dict):
        size += sum((get_total_size(k, seen) + get_total_size(v, seen)) for k, v in obj.items())
    elif isinstance(obj, (list, tuple, set, frozenset)):
        size += sum(get_total_size(item, seen) for item in obj)
    elif isinstance(obj, np.ndarray):
        # Numpy array, account for the full memory usage of the array
        size += obj.nbytes

    return size


def combine_seeds_and_envs(x: jnp.ndarray):
    # Here, dim=-1 is the NUM_ENVS parameter. We take the mean over this.
    # dim=-2 is the NUM_STEPS parameter.
    # dim=-3 is the NUM_UPDATES, which is TOTAL_TIMESTEPS // NUM_STEPS // NUM_ENVS.
    # dim=-4 is n_seeds.
    # We take the mean and std_err to the mean over dimensions -1 and -4.
    envs_seeds_swapped = jnp.swapaxes(x, -2, -4).swapaxes(-3, -4)

    # We take the mean over NUM_ENVS dimension.
    mean_over_num_envs = envs_seeds_swapped.mean(axis=-1)
    return mean_over_num_envs


def get_first_returns(returned_episode: jnp.ndarray, returns: jnp.ndarray):
    first_episode_ends = returned_episode.argmax(axis=-2)

    mesh_inputs = [np.arange(dim) for dim in first_episode_ends.shape]
    grids = np.meshgrid(*mesh_inputs, indexing='ij')
    grids_before, grids_after = grids[:-1], grids[-1:]
    return returns[(*grids_before, first_episode_ends, *grids_after)]


def get_final_eval(final_eval: dict):
    # Get final eval metrics
    disc_returns = final_eval['returned_discounted_episode_returns']

    first_episode_ends = final_eval['returned_episode'].argmax(axis=-2)

    mesh_inputs = [np.arange(dim) for dim in first_episode_ends.shape]
    grids = np.meshgrid(*mesh_inputs, indexing='ij')
    grids_before, grids_after = grids[:-1], grids[-1:]
    final_first_disc_returns = disc_returns[(*grids_before, first_episode_ends, *grids_after)]
    return final_first_disc_returns


def parse_exp_dir(study_path, study_hparam_path):
    # TODO: refactor this to use the entry point and see the arguments to the `train` function.
    train_sign_hparams = ['ld_weight', 'alpha', 'lambda1', 'lambda0', 'lr']

    spec = importlib.util.spec_from_file_location('temp', study_hparam_path)
    var_module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(var_module)

    # here we assume all variables that aren't Path or
    # 'hparams' or 'exp_name'
    swept_hparams = OrderedDict([(hp, getattr(var_module, hp + 's')) for hp in train_sign_hparams])
    missing_hparams = OrderedDict()

    all_list_hparams = getattr(var_module, 'hparams')['args']

    different_hyperparams = {}

    if len(all_list_hparams) > 1:
        # Here we assume all keys are the same across all hparams.
        ex_hyperparams = all_list_hparams[0]
        all_hparams = ex_hyperparams.copy()
        for hparams in all_list_hparams:
            for k, v in hparams.items():
                assert k in ex_hyperparams, "Key isn't the same for all hparams."
                # if our values are different, that means we need to concatenate them later on.
                if v != ex_hyperparams[k]:
                    if k not in different_hyperparams:
                        different_hyperparams[k] = getattr(var_module, k + 's')
                        all_hparams[k] = ' '.join(map(str, different_hyperparams[k]))
                    else:
                        assert v in different_hyperparams[k]
    else:
        all_hparams = all_list_hparams[0]

    # here we need to see if we manually sweep over hparams,
    # instead of vmapping over that hparam.
    for k, v in all_hparams.items():
        if isinstance(v, list) and k != 'env':
            missing_hparams[k] = v

    env_order = all_hparams['env']

    study_paths = [s for s in study_path.iterdir() if s.is_dir()]

    envs = []
    scores_by_env = {}

    for results_path in tqdm(study_paths):
        orbax_checkpointer = orbax.checkpoint.PyTreeCheckpointer()

        restored = orbax_checkpointer.restore(results_path)

        # Make sure the swept hypers are the same order as listed above.
        # Last argument is rng.
        swept_order = restored['argument_order']

        if swept_order is not None:
            for i, k in enumerate(swept_order[:-1]):
                assert k == train_sign_hparams[i]

        config, args = restored['config'], restored['args']

        # Get online metrics
        online_eval = restored['out']['metric']
        online_disc_returns = online_eval['returned_discounted_episode_returns']

        final_eval = restored['out']['final_eval_metric']
        # we take the mean over axis=-2 here, since this dimension might be different
        # for the final eval.
        final_n_episodes = final_eval['returned_episode'].sum(axis=-2, keepdims=True)
        final_disc_returns = final_eval['returned_discounted_episode_returns'].sum(axis=-2, keepdims=True)
        final_disc_returns /= (final_n_episodes + (final_n_episodes == 0).astype(float))  # add the 0 mask to prevent division by 0.

        # we add a num_updates dimension
        final_disc_returns = np.expand_dims(final_disc_returns, -3)

        del restored
        seeds_combined = combine_seeds_and_envs(online_disc_returns)

        final_seeds_combined = combine_seeds_and_envs(final_disc_returns)

        # TODO: we want to deal with missing hparams and lists of hparams in similar ways
        if len(missing_hparams) > 0:

            arg_vs = []
            for k in missing_hparams:
                v = args[k]
                if isinstance(v, np.ndarray):
                    assert v.shape[0] == 1
                    v = v.item()
                arg_vs.append((k, v))

            missing_id = tuple(arg_vs)
            if args['env'] not in scores_by_env:
                scores_by_env[args['env']] = {}

            if missing_id not in scores_by_env[args['env']]:
                scores_by_env[args['env']][missing_id] = []

            scores_by_env[args['env']][missing_id].append((args, seeds_combined, final_seeds_combined))
        else:
            if args['env'] not in scores_by_env:
                scores_by_env[args['env']] = []
            scores_by_env[args['env']].append((args, seeds_combined, final_seeds_combined))

    # Now we deal with lists of hyperparams
    def process_group(group: list[dict], diff_hyperparams: dict):
        """
        This function processes a list of dictionaries
        according to diff_hyperparams, BEFORE we do the
        concatenation for missing_hyperparams.
        Returns a concatenated array, with arg_index ordered by
        the order in diff_hyperparams
        """
        if not diff_hyperparams:
            assert len(group) == 1
            return group[0][-2], group[0][-1]

        if len(diff_hyperparams.keys()) == 1:
            # parse the last hyperparam
            arg, order = next(iter(diff_hyperparams.items()))

            arg_index = train_sign_hparams.index(arg)

            ordered_scores = []
            ordered_final_scores = []
            for v in order:
                for args, seeds_combined, final_seeds_combined in group:
                    if args[arg] == v:
                        ordered_scores.append(seeds_combined)
                        ordered_final_scores.append(final_seeds_combined)
                        break
                else:
                    raise IndexError(f"{args} Not found!")
            return jnp.concatenate(ordered_scores, axis=arg_index), jnp.concatenate(ordered_final_scores, axis=arg_index)

        raise NotImplementedError

    for env, env_dict in scores_by_env.items():
        if len(missing_hparams) > 0:
            for missing_id, missing_dict in env_dict.items():
                scores_by_env[env][missing_id] = process_group(missing_dict, different_hyperparams)
        else:
            scores_by_env[env] = process_group(env_dict, different_hyperparams)

    def find_close_idx(list_els, el):
        """
        We use this function b/c there's weird floating
        point errors in np.ndarray.item().
        """
        if isinstance(el, float):
            for i, e in enumerate(list_els):
                if np.isclose(e, el, atol=1e-8):
                    return i
        else:
            return list_els.index(el)

    # we add new dimensions for each swept over hparam.
    # if the hparam exists in train_sign_hparams,
    def get_reduced_score_dict(score_dict, k):
        filtered_keys = set()
        for missing_id in score_dict.keys():
            filtered_keys.add(tuple(sorted((name, val) for name, val in missing_id if name != k)))

        return list(filtered_keys)

    if len(missing_hparams) > 0:
        to_delete_keys = []
        for env, score_dict in scores_by_env.items():

            # we do reversed here to match the prepending of dimensions
            for k, v in reversed(missing_hparams.items()):
                new_score_keys = get_reduced_score_dict(score_dict, k)
                new_score_dict = {k:
                                      {'grouped_reses': [None] * len(v),
                                       'final_grouped_reses': [None] * len(v)}
                                  for k in new_score_keys}

                for id, (scores, final_scores) in score_dict.items():
                    new_score_key = tuple(sorted((name, val) for name, val in id if name != k))
                    for kp, vp in id:
                        if kp == k:
                            idx = find_close_idx(v, vp)
                            new_score_dict[new_score_key]['grouped_reses'][idx] = scores
                            new_score_dict[new_score_key]['final_grouped_reses'][idx] = final_scores
                            break
                    else:
                        raise KeyError(f'{k} not in {id}')

                score_dict = {}
                # we're done sorting the hyperparams here.
                # Now we have to stack them, either to an existing dimension
                # or to a new dimension
                if k in train_sign_hparams:
                    k_idx = train_sign_hparams.index(k)
                    for new_score_key, new_score in new_score_dict.items():
                        score_dict[new_score_key] = (
                            np.concatenate(new_score['grouped_reses'], axis=k_idx),
                            np.concatenate(new_score['final_grouped_reses'], axis=k_idx)
                        )

                    # we delete here so we don't concatenate this to the dim_ref
                    if k not in to_delete_keys:
                        to_delete_keys.append(k)
                else:
                    for new_score_key, new_score in new_score_dict.items():
                        score_dict[new_score_key] = (
                            np.stack(new_score['grouped_reses'], axis=0),
                            np.stack(new_score['final_grouped_reses'], axis=0)
                        )

            scores_by_env[env] = score_dict[()]

        if to_delete_keys:
            for k in to_delete_keys:
                del missing_hparams[k]
        # add our missing hyperparams
        swept_hparams = OrderedDict(list(missing_hparams.items()) + list(swept_hparams.items()))

    # Stack by envs
    envs = []
    scores = []
    final_scores = []
    for k, (v, final_v) in scores_by_env.items():
        envs.append(k)
        scores.append(v)
        final_scores.append(final_v)
    stacked_scores = np.stack(scores, axis=-1)
    stacked_final_scores = np.stack(final_scores, axis=-1)

    dim_ref = [*swept_hparams, 'num_update', 'num_steps', 'seeds', 'env']

    parsed_res = {
        'envs': envs,
        'scores': stacked_scores,
        'final_scores': stacked_final_scores,
        'dim_ref': dim_ref,
        'hyperparams': swept_hparams
    }
    return parsed_res

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument('study_path', type=str)
    args = parser.parse_args()

    study_path = Path(args.study_path)
    study_hparam_path = Path(ROOT_DIR, 'scripts', 'hyperparams', study_path.stem + '.py')

    parsed_res_path = study_path / "parsed_hparam_scores.pkl"

    parsed_res = parse_exp_dir(study_path, study_hparam_path)

    print(f"Saving parsed results to {parsed_res_path}")
    with open(parsed_res_path, 'wb') as f:
        pickle.dump(parsed_res, f, protocol=4)
